{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from sklearn import base\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "past = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict = pd.read_csv('data/conflict.csv')\n",
    "pred = pd.read_csv('data/pak_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict['event_date'] =  pd.to_datetime(conflict['event_date'], format='%d %B %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict['Battles'] = conflict['event_type'] == 'Battles'\n",
    "conflict['Explosions'] = conflict['event_type'] == 'Explosions/Remote violence'\n",
    "conflict['Protests'] = conflict['event_type'] == 'Protests'\n",
    "conflict['Riots'] = conflict['event_type'] == 'Riots'\n",
    "conflict['Strategic developments'] = conflict['event_type'] == 'Strategic developments'\n",
    "conflict['Violence against civilians'] = conflict['event_type'] == 'Violence against civilians'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict['month_year'] = conflict.event_date.values.astype('datetime64[M]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict  = conflict[['month_year','latitude','longitude','Battles','Explosions','Protests','Riots',\n",
    "                      'Strategic developments','Violence against civilians','fatalities','admin1','admin2','admin3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict['fatalities_battles'] = conflict['fatalities'] * conflict['Battles']\n",
    "conflict['fatalities_explosions'] = conflict['fatalities'] * conflict['Explosions']\n",
    "conflict['fatalities_protests'] = conflict['fatalities'] * conflict['Protests']\n",
    "conflict['fatalities_riots'] = conflict['fatalities'] * conflict['Riots']\n",
    "conflict['fatalities_strategic'] = conflict['fatalities'] * conflict['Strategic developments']\n",
    "conflict['fatalities_civilian'] = conflict['fatalities'] * conflict['Violence against civilians']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict = conflict.groupby(['month_year','latitude','longitude']).agg({'Battles':'sum',\n",
    "                                                             'Explosions':'sum',\n",
    "                                                             'Protests':'sum',\n",
    "                                                             'Riots':'sum',\n",
    "                                                             'Strategic developments':'sum',\n",
    "                                                             'Violence against civilians':'sum',\n",
    "                                                             'fatalities_battles':'sum',\n",
    "                                                             'fatalities_explosions':'sum',\n",
    "                                                             'fatalities_protests':'sum',\n",
    "                                                             'fatalities_riots':'sum',\n",
    "                                                             'fatalities_strategic':'sum',\n",
    "                                                             'fatalities_civilian':'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.loc[:, ~pred.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.rename(columns={\"x\": \"longitude\", \"y\": \"latitude\", \"pak_ppp_2015\": \"pop\", \"pak07povmpi\": \"pov\", \n",
    "                     \"F182013.v4c_web.stable_lights.avg_vis\": \"nl\", \"index_1\": \"temp\", \"layer\": \"elev\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.dropna(subset=['pop', 'pov','nl','temp','elev']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(pred[['longitude','latitude']],conflict, on = ['longitude','latitude'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[X.month_year.isnull(),['month_year']] = pd.to_datetime('2010-01-01')\n",
    "X = X[X['month_year'] != pd.to_datetime('2019-10-01')]\n",
    "X['month_year'] = X['month_year'].dt.strftime('%Y-%m')\n",
    "X = X.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.sort_values(by = ['longitude','latitude','month_year']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('2010-01-01','2019-09-01', \n",
    "              freq='MS').strftime(\"%Y-%m\").tolist()\n",
    "coor = pred[['longitude','latitude']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [coor,dates]\n",
    "data = list(itertools.product(*a))\n",
    "df = pd.DataFrame(data, columns=['coor','month_year'])\n",
    "df[['longitude','latitude']] = pd.DataFrame(df.coor.values.tolist(), index= df.index)\n",
    "df = df[['longitude','latitude','month_year']].sort_values(by = ['longitude','latitude','month_year']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(df,X, on = ['longitude','latitude','month_year'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(pred,X, on = ['longitude','latitude'], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['y'] = X.apply(lambda row: int((row['Battles'] + row['Explosions'] + row['Riots'] \n",
    "                                  + row['Violence against civilians']) != 0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['month'] = X.apply(lambda row: int(row['month_year'].split('-')[1]), axis=1)\n",
    "X['year'] = X.apply(lambda row: int(row['month_year'].split('-')[0]), axis=1)\n",
    "X['year'] = X['year'] - 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['y_next'] = X.shift(-1).y\n",
    "X.loc[X.index.to_series().groupby([X['longitude'],X['latitude']]).last().reset_index(name='x')['x'],'y_next'] = np.nan\n",
    "\n",
    "y = X['y_next'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers and estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    def __init__(self, col_names):\n",
    "        self.col_names = col_names  # We will need these in transform()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        features = X[self.col_names].values\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_transformer = ColumnTransformer([(\"onehot\", OneHotEncoder(categories='auto', sparse=False),slice(19,20))], \n",
    "                                        remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = ColumnTransformer([(\"standard\", StandardScaler(),slice(12,19))], \n",
    "                                        remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "    \n",
    "    beta = 1.1\n",
    "\n",
    "    f1 = (1 + (beta**2))*p*r / ((beta**2) * p + r + K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnEstimator(base.BaseEstimator, base.ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, past, loss, metrics, opt, epochs):\n",
    "        self.past = past\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.opt = opt\n",
    "        self.model = Sequential()\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X_list = X.tolist()\n",
    "        y_list = y.tolist()\n",
    "        n = len(X_list)\n",
    "        y_list = [y_list[i:i+self.past] for i in range(n - self.past + 1)]\n",
    "        X_list = [X_list[i:i+self.past] for i in range(n - self.past + 1)]\n",
    "        \n",
    "        to_remove = [i for i in range(len(y_list)) if True in [math.isnan(item) for item in y_list[i]]]\n",
    "\n",
    "        for index in sorted(to_remove, reverse=True):\n",
    "            del y_list[index]\n",
    "            del X_list[index]\n",
    "        y_list = [int(item[-1]) for item in y_list]\n",
    "        \n",
    "        y = np.array(y_list)\n",
    "        X = np.array(X_list)\n",
    "        \n",
    "        n_steps = X.shape[1]\n",
    "        n_features = X.shape[2]\n",
    "        \n",
    "        self.model.add(LSTM(10, input_shape=(n_steps, n_features)))\n",
    "        self.model.add(Dense(5, activation='tanh'))\n",
    "        self.model.add(Dense(10, activation='tanh'))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        self.model.compile(optimizer=self.opt, loss=self.loss, metrics=self.metrics)\n",
    "        \n",
    "        self.model.fit(X, y, epochs=self.epochs)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        X_list = X.tolist()\n",
    "        y_list = y.tolist()\n",
    "        n = len(X_list)\n",
    "        y_list = [y_list[i:i+self.past] for i in range(n - self.past + 1)]\n",
    "        X_list = [X_list[i:i+self.past] for i in range(n - self.past + 1)]\n",
    "        \n",
    "        to_remove = [i for i in range(len(y_list)) if True in [math.isnan(item) for item in y_list[i]]]\n",
    "\n",
    "        for index in sorted(to_remove, reverse=True):\n",
    "            del y_list[index]\n",
    "            del X_list[index]\n",
    "        y_list = [int(item[-1]) for item in y_list]\n",
    "        \n",
    "        y = np.array(y_list)\n",
    "        X = np.array(X_list)\n",
    "        \n",
    "        return self.model.evaluate(X, y, batch_size = X.shape[0])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_list = X.tolist()\n",
    "        n = len(X_list)\n",
    "        X_list = [X_list[i:i+self.past] for i in range(0, n - self.past +1, self.past)]\n",
    "        X = np.array(X_list)\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[X['month_year'] < '2017-07'].copy()\n",
    "X_test = X[X['month_year'] >= '2017-07'].copy()\n",
    "\n",
    "X_train['y_next'] = X_train.shift(-1).y\n",
    "X_train.loc[X_train.index.to_series().groupby([X_train['longitude'], \n",
    "                                               X_train['latitude']]).last().reset_index(name='x')['x'],'y_next'] = np.nan\n",
    "\n",
    "X_test['y_next'] = X_test.shift(-1).y\n",
    "X_test.loc[X_test.index.to_series().groupby([X_test['longitude'], \n",
    "                                             X_test['latitude']]).last().reset_index(name='x')['x'],'y_next'] = np.nan\n",
    "\n",
    "y_train = X_train['y_next'].values\n",
    "\n",
    "y_test = X_test['y_next'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "pipe = Pipeline([\n",
    "        (\"columns\", ColumnSelectTransformer(['longitude','latitude','pop','pov','nl','temp','elev','Battles','Explosions',\n",
    "             'Protests','Riots','Strategic developments','Violence against civilians',\n",
    "             'fatalities_battles','fatalities_explosions','fatalities_protests','fatalities_riots',\n",
    "             'fatalities_strategic','fatalities_civilian','month','year','y'])),\n",
    "        (\"one_hot\", one_hot_transformer),\n",
    "        (\"scale\",scaler),\n",
    "        (\"rnn\", RnnEstimator(past, f_loss, [precision, recall, 'accuracy'], opt, 20))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8658/8658 [==============================] - 4s 468us/sample - loss: 0.3584 - precision: 0.5742 - recall: 0.7862 - acc: 0.7976\n",
      "Epoch 2/20\n",
      "8658/8658 [==============================] - 3s 355us/sample - loss: 0.3121 - precision: 0.6219 - recall: 0.7871 - acc: 0.8266\n",
      "Epoch 3/20\n",
      "8658/8658 [==============================] - 3s 332us/sample - loss: 0.3074 - precision: 0.6194 - recall: 0.8064 - acc: 0.8283\n",
      "Epoch 4/20\n",
      "8658/8658 [==============================] - 3s 364us/sample - loss: 0.3100 - precision: 0.6141 - recall: 0.8093 - acc: 0.8254\n",
      "Epoch 5/20\n",
      "8658/8658 [==============================] - 3s 353us/sample - loss: 0.2978 - precision: 0.6222 - recall: 0.8201 - acc: 0.8309\n",
      "Epoch 6/20\n",
      "8658/8658 [==============================] - 3s 374us/sample - loss: 0.2985 - precision: 0.6206 - recall: 0.8200 - acc: 0.8292\n",
      "Epoch 7/20\n",
      "8658/8658 [==============================] - 3s 389us/sample - loss: 0.2972 - precision: 0.6209 - recall: 0.8215 - acc: 0.8314\n",
      "Epoch 8/20\n",
      "8658/8658 [==============================] - 3s 349us/sample - loss: 0.2976 - precision: 0.6289 - recall: 0.8096 - acc: 0.8341\n",
      "Epoch 9/20\n",
      "8658/8658 [==============================] - 4s 420us/sample - loss: 0.2918 - precision: 0.6310 - recall: 0.8172 - acc: 0.8336\n",
      "Epoch 10/20\n",
      "8658/8658 [==============================] - 3s 349us/sample - loss: 0.2914 - precision: 0.6295 - recall: 0.8213 - acc: 0.8348\n",
      "Epoch 11/20\n",
      "8658/8658 [==============================] - 3s 316us/sample - loss: 0.2955 - precision: 0.6292 - recall: 0.8194 - acc: 0.8360\n",
      "Epoch 12/20\n",
      "8658/8658 [==============================] - 3s 325us/sample - loss: 0.2916 - precision: 0.6315 - recall: 0.8205 - acc: 0.8361\n",
      "Epoch 13/20\n",
      "8658/8658 [==============================] - 3s 353us/sample - loss: 0.2938 - precision: 0.6294 - recall: 0.8219 - acc: 0.8350\n",
      "Epoch 14/20\n",
      "8658/8658 [==============================] - 3s 334us/sample - loss: 0.2921 - precision: 0.6293 - recall: 0.8206 - acc: 0.8354\n",
      "Epoch 15/20\n",
      "8658/8658 [==============================] - 3s 347us/sample - loss: 0.2883 - precision: 0.6362 - recall: 0.8239 - acc: 0.8373\n",
      "Epoch 16/20\n",
      "8658/8658 [==============================] - 3s 363us/sample - loss: 0.2910 - precision: 0.6337 - recall: 0.8199 - acc: 0.8367\n",
      "Epoch 17/20\n",
      "8658/8658 [==============================] - 3s 382us/sample - loss: 0.2907 - precision: 0.6335 - recall: 0.8230 - acc: 0.8373\n",
      "Epoch 18/20\n",
      "8658/8658 [==============================] - 3s 339us/sample - loss: 0.2889 - precision: 0.6349 - recall: 0.8216 - acc: 0.8377\n",
      "Epoch 19/20\n",
      "8658/8658 [==============================] - 3s 336us/sample - loss: 0.2894 - precision: 0.6377 - recall: 0.8172 - acc: 0.8377\n",
      "Epoch 20/20\n",
      "8658/8658 [==============================] - 3s 374us/sample - loss: 0.2872 - precision: 0.6381 - recall: 0.8221 - acc: 0.8389\n"
     ]
    }
   ],
   "source": [
    "rnn_pipe = pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "8658/8658 [==============================] - 0s 42us/sample - loss: 0.2749 - precision: 0.6368 - recall: 0.8220 - acc: 0.8392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.274935781955719, 0.6367891, 0.822026, 0.83922386]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1665/1665 [==============================] - 0s 9us/sample - loss: 0.2167 - precision: 0.7849 - recall: 0.7817 - acc: 0.8691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21673840284347534, 0.78486055, 0.78174603, 0.86906904]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = X[X['month_year'] < '2019-09'].copy()\n",
    "\n",
    "X_full['y_next'] = X_train.shift(-1).y\n",
    "X_full.loc[X_train.index.to_series().groupby([X_train['longitude'], \n",
    "                                               X_train['latitude']]).last().reset_index(name='x')['x'],'y_next'] = np.nan\n",
    "\n",
    "y_full = X_full['y_next'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "pipe = Pipeline([\n",
    "        (\"columns\", ColumnSelectTransformer(['longitude','latitude','pop','pov','nl','temp','elev','Battles','Explosions',\n",
    "             'Protests','Riots','Strategic developments','Violence against civilians',\n",
    "             'fatalities_battles','fatalities_explosions','fatalities_protests','fatalities_riots',\n",
    "             'fatalities_strategic','fatalities_civilian','month','year'])),\n",
    "        (\"one_hot\", one_hot_transformer),\n",
    "        (\"scale\",scaler),\n",
    "        (\"rnn\", RnnEstimator(past, f_loss, [precision, recall, 'accuracy'], opt, 40))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "11655/11655 [==============================] - 5s 433us/sample - loss: 0.3390 - precision: 0.6098 - recall: 0.7638 - acc: 0.8152\n",
      "Epoch 2/40\n",
      "11655/11655 [==============================] - 4s 342us/sample - loss: 0.3125 - precision: 0.6249 - recall: 0.7827 - acc: 0.8282\n",
      "Epoch 3/40\n",
      "11655/11655 [==============================] - 4s 360us/sample - loss: 0.3092 - precision: 0.6316 - recall: 0.7804 - acc: 0.8312\n",
      "Epoch 4/40\n",
      "11655/11655 [==============================] - 4s 385us/sample - loss: 0.3071 - precision: 0.6432 - recall: 0.7779 - acc: 0.8358\n",
      "Epoch 5/40\n",
      "11655/11655 [==============================] - 5s 393us/sample - loss: 0.3017 - precision: 0.6411 - recall: 0.7840 - acc: 0.8354\n",
      "Epoch 6/40\n",
      "11655/11655 [==============================] - 5s 399us/sample - loss: 0.2978 - precision: 0.6493 - recall: 0.7798 - acc: 0.8383\n",
      "Epoch 7/40\n",
      "11655/11655 [==============================] - 5s 409us/sample - loss: 0.2961 - precision: 0.6498 - recall: 0.7878 - acc: 0.8384\n",
      "Epoch 8/40\n",
      "11655/11655 [==============================] - 4s 365us/sample - loss: 0.2946 - precision: 0.6485 - recall: 0.7930 - acc: 0.8379\n",
      "Epoch 9/40\n",
      "11655/11655 [==============================] - 4s 356us/sample - loss: 0.2958 - precision: 0.6441 - recall: 0.7922 - acc: 0.8379\n",
      "Epoch 10/40\n",
      "11655/11655 [==============================] - 4s 375us/sample - loss: 0.2923 - precision: 0.6465 - recall: 0.7942 - acc: 0.8386\n",
      "Epoch 11/40\n",
      "11655/11655 [==============================] - 5s 394us/sample - loss: 0.2882 - precision: 0.6599 - recall: 0.7949 - acc: 0.8410\n",
      "Epoch 12/40\n",
      "11655/11655 [==============================] - 5s 388us/sample - loss: 0.2921 - precision: 0.6499 - recall: 0.7960 - acc: 0.8401\n",
      "Epoch 13/40\n",
      "11655/11655 [==============================] - 5s 387us/sample - loss: 0.2925 - precision: 0.6521 - recall: 0.7952 - acc: 0.8422\n",
      "Epoch 14/40\n",
      "11655/11655 [==============================] - 4s 376us/sample - loss: 0.2881 - precision: 0.6509 - recall: 0.7963 - acc: 0.8402\n",
      "Epoch 15/40\n",
      "11655/11655 [==============================] - 5s 388us/sample - loss: 0.2906 - precision: 0.6525 - recall: 0.7935 - acc: 0.8426\n",
      "Epoch 16/40\n",
      "11655/11655 [==============================] - 4s 385us/sample - loss: 0.2892 - precision: 0.6564 - recall: 0.7948 - acc: 0.8426\n",
      "Epoch 17/40\n",
      "11655/11655 [==============================] - 4s 362us/sample - loss: 0.2904 - precision: 0.6522 - recall: 0.7949 - acc: 0.8433\n",
      "Epoch 18/40\n",
      "11655/11655 [==============================] - 4s 372us/sample - loss: 0.2911 - precision: 0.6499 - recall: 0.7965 - acc: 0.8429\n",
      "Epoch 19/40\n",
      "11655/11655 [==============================] - 4s 367us/sample - loss: 0.2871 - precision: 0.6547 - recall: 0.7983 - acc: 0.8422\n",
      "Epoch 20/40\n",
      "11655/11655 [==============================] - 4s 333us/sample - loss: 0.2831 - precision: 0.6590 - recall: 0.8054 - acc: 0.8430\n",
      "Epoch 21/40\n",
      "11655/11655 [==============================] - 4s 369us/sample - loss: 0.2850 - precision: 0.6619 - recall: 0.8002 - acc: 0.8438\n",
      "Epoch 22/40\n",
      "11655/11655 [==============================] - 4s 369us/sample - loss: 0.2894 - precision: 0.6547 - recall: 0.7972 - acc: 0.8444\n",
      "Epoch 23/40\n",
      "11655/11655 [==============================] - 5s 406us/sample - loss: 0.2836 - precision: 0.6616 - recall: 0.8010 - acc: 0.8444\n",
      "Epoch 24/40\n",
      "11655/11655 [==============================] - 4s 357us/sample - loss: 0.2872 - precision: 0.6585 - recall: 0.7952 - acc: 0.8447\n",
      "Epoch 25/40\n",
      "11655/11655 [==============================] - 4s 356us/sample - loss: 0.2884 - precision: 0.6565 - recall: 0.7988 - acc: 0.8450\n",
      "Epoch 26/40\n",
      "11655/11655 [==============================] - 5s 404us/sample - loss: 0.2886 - precision: 0.6540 - recall: 0.7922 - acc: 0.8451\n",
      "Epoch 27/40\n",
      "11655/11655 [==============================] - 5s 413us/sample - loss: 0.2846 - precision: 0.6575 - recall: 0.8022 - acc: 0.8448\n",
      "Epoch 28/40\n",
      "11655/11655 [==============================] - 5s 421us/sample - loss: 0.2842 - precision: 0.6594 - recall: 0.8035 - acc: 0.8450\n",
      "Epoch 29/40\n",
      "11655/11655 [==============================] - 4s 366us/sample - loss: 0.2807 - precision: 0.6601 - recall: 0.8012 - acc: 0.8450\n",
      "Epoch 30/40\n",
      "11655/11655 [==============================] - 4s 354us/sample - loss: 0.2854 - precision: 0.6601 - recall: 0.8002 - acc: 0.8452\n",
      "Epoch 31/40\n",
      "11655/11655 [==============================] - 4s 373us/sample - loss: 0.2864 - precision: 0.6610 - recall: 0.7935 - acc: 0.8452\n",
      "Epoch 32/40\n",
      "11655/11655 [==============================] - 4s 378us/sample - loss: 0.2830 - precision: 0.6597 - recall: 0.8045 - acc: 0.8453\n",
      "Epoch 33/40\n",
      "11655/11655 [==============================] - 4s 369us/sample - loss: 0.2848 - precision: 0.6600 - recall: 0.7968 - acc: 0.8451\n",
      "Epoch 34/40\n",
      "11655/11655 [==============================] - 4s 368us/sample - loss: 0.2860 - precision: 0.6604 - recall: 0.7994 - acc: 0.8460\n",
      "Epoch 35/40\n",
      "11655/11655 [==============================] - 4s 377us/sample - loss: 0.2828 - precision: 0.6609 - recall: 0.7962 - acc: 0.8459\n",
      "Epoch 36/40\n",
      "11655/11655 [==============================] - 5s 404us/sample - loss: 0.2849 - precision: 0.6579 - recall: 0.7987 - acc: 0.8453\n",
      "Epoch 37/40\n",
      "11655/11655 [==============================] - 4s 377us/sample - loss: 0.2850 - precision: 0.6604 - recall: 0.7962 - acc: 0.8459\n",
      "Epoch 38/40\n",
      "11655/11655 [==============================] - 4s 367us/sample - loss: 0.2859 - precision: 0.6557 - recall: 0.7955 - acc: 0.8459\n",
      "Epoch 39/40\n",
      "11655/11655 [==============================] - 4s 364us/sample - loss: 0.2860 - precision: 0.6614 - recall: 0.7974 - acc: 0.8462\n",
      "Epoch 40/40\n",
      "11655/11655 [==============================] - 5s 394us/sample - loss: 0.2852 - precision: 0.6584 - recall: 0.7975 - acc: 0.8463\n"
     ]
    }
   ],
   "source": [
    "rnn_pipe = pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "11655/11655 [==============================] - 0s 40us/sample - loss: 0.2708 - precision: 0.6619 - recall: 0.7995 - acc: 0.8465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2707822918891907, 0.66187656, 0.7995235, 0.8465037]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_pipe.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_naive = X.copy()\n",
    "\n",
    "X_naive = X_naive.dropna(subset=['y_next']).reset_index(drop = True)\n",
    "\n",
    "true_positives = sum((X_naive['y_next'] * X_naive['y']).values)\n",
    "predicted_positives = sum(X_naive['y'].values)\n",
    "possible_positives = sum((X_naive['y_next']).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6294670846394984"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positives / predicted_positives #precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6275"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positives / possible_positives #recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for September 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = X.copy()\n",
    "X_pred = X_pred[(X_pred['month_year'] >= '2018-09') & (X_pred['month_year'] < '2019-09')].reset_index(drop=True)\n",
    "\n",
    "predictions = rnn_pipe.predict(X_pred)\n",
    "lat_lon = X_pred.iloc[::12, :].reset_index(drop=True)[['longitude','latitude']]\n",
    "lat_lon['pred'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Predictions for September 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = conflict[conflict['month_year'] == pd.to_datetime('2019-08-01')]\n",
    "naive = pd.merge(pred[['longitude','latitude']],naive, on = ['longitude','latitude'], how = 'left')\n",
    "naive.loc[naive.month_year.isnull(),['month_year']] = pd.to_datetime('2019-08-01')\n",
    "naive['month_year'] = naive['month_year'].dt.strftime('%Y-%m')\n",
    "naive = naive.fillna(0)\n",
    "naive = naive.sort_values(by = ['longitude','latitude','month_year']).reset_index(drop=True)\n",
    "naive['naive_pred'] = naive.apply(lambda row: int((row['Battles'] + row['Explosions'] + row['Riots'] \n",
    "                                                   + row['Violence against civilians']) != 0), axis=1)\n",
    "\n",
    "naive = naive[['longitude','latitude','naive_pred']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual values for September 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = conflict[conflict['month_year'] == pd.to_datetime('2019-09-01')]\n",
    "actual = pd.merge(pred[['longitude','latitude']],actual, on = ['longitude','latitude'], how = 'left')\n",
    "actual.loc[actual.month_year.isnull(),['month_year']] = pd.to_datetime('2019-09-01')\n",
    "actual['month_year'] = actual['month_year'].dt.strftime('%Y-%m')\n",
    "actual = actual.fillna(0)\n",
    "actual = actual.sort_values(by = ['longitude','latitude','month_year']).reset_index(drop=True)\n",
    "actual['actual'] = actual.apply(lambda row: int((row['Battles'] + row['Explosions'] + row['Riots'] \n",
    "                                                 + row['Violence against civilians']) != 0), axis=1)\n",
    "\n",
    "actual = actual[['longitude','latitude','actual']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = pd.merge(lat_lon, naive, on = ['longitude','latitude'])\n",
    "compare = pd.merge(compare, actual, on = ['longitude','latitude'])\n",
    "\n",
    "true_positives = sum((compare['actual'] * (compare['pred'] > 0.5).astype('int')).values)\n",
    "predicted_positives = sum(((compare['pred'] > 0.5).astype('int')).values)\n",
    "possible_positives = sum((compare['actual']).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare.to_csv(\"data/predictions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision and Recall for RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positives / predicted_positives #precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positives / possible_positives #recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision and Recall for Naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = sum((compare['actual'] * compare['naive_pred']).values)\n",
    "predicted_positives = sum((compare['naive_pred']).values)\n",
    "possible_positives = sum((compare['actual']).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positives / predicted_positives #precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7878787878787878"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positives / possible_positives #recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone Development",
   "language": "python",
   "name": "capstone-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
